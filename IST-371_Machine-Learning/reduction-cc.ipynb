{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IST 317 Machine Learning\n",
    "\n",
    "## Spring 2019\n",
    "\n",
    "### Chapter 8 assignment\n",
    "\n",
    "#### Aidan Polivka\n",
    "\n",
    "\n",
    "In this assignment, we will return once again to the credit card dataset. This time we will focus on reducing the dimensionality of the dataset, using both PCA and LLE methods. \n",
    "\n",
    "First, load the `creditcard.csv` dataset as a Pandas dataframe. Then, separate it into training features `X`, training labels `y`, testing features `X_test`, and testing labels `y_test`. Finally, scale the feature data with a `StandardScaler`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         V8        V9  ...         V21       V22       V23       V24  \\\n",
       "0  0.098698  0.363787  ...   -0.018307  0.277838 -0.110474  0.066928   \n",
       "1  0.085102 -0.255425  ...   -0.225775 -0.638672  0.101288 -0.339846   \n",
       "2  0.247676 -1.514654  ...    0.247998  0.771679  0.909412 -0.689281   \n",
       "3  0.377436 -1.387024  ...   -0.108300  0.005274 -0.190321 -1.175575   \n",
       "4 -0.270533  0.817739  ...   -0.009431  0.798278 -0.137458  0.141267   \n",
       "\n",
       "        V25       V26       V27       V28  Amount  Class  \n",
       "0  0.128539 -0.189115  0.133558 -0.021053  149.62      0  \n",
       "1  0.167170  0.125895 -0.008983  0.014724    2.69      0  \n",
       "2 -0.327642 -0.139097 -0.055353 -0.059752  378.66      0  \n",
       "3  0.647376 -0.221929  0.062723  0.061458  123.50      0  \n",
       "4 -0.206010  0.502292  0.219422  0.215153   69.99      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the file into a Pandas dataframe and display the frame\n",
    "# Import Pandas\n",
    "import pandas as pds\n",
    "\n",
    "#Super secret Magic Words of DOOM\n",
    "%matplotlib inline\n",
    "\n",
    "#Load in our Data\n",
    "cardinfo = pds.read_csv('creditcard.csv')  \n",
    "\n",
    "#Show some header data\n",
    "cardinfo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separate the data into training and testing features and labels\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "# use StratifiedShuffleSplit to create a training set (80% of the data)\n",
    "# and a testing set (20% of the data)\n",
    "split = StratifiedShuffleSplit(n_splits = 1, test_size = 0.2, random_state = 68333)\n",
    "for train_index, test_index in split.split(cardinfo, cardinfo['Class']):\n",
    "    train_set = cardinfo.loc[train_index]\n",
    "    test_set = cardinfo.loc[test_index]\n",
    "\n",
    "# create X and X_test, with all the features except 'Class'\n",
    "X_test = train_set.drop('Class', axis = 1)\n",
    "X=train_set.drop('Class',axis=1)\n",
    "\n",
    "\n",
    "# create y and y_test, with only the 'Class' values\n",
    "y_test = train_set['Class'].copy()\n",
    "y = train_set['Class'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "#Feature the data with a standardscaler\n",
    "# use StandardScaler to scale X and X_test\n",
    "scaler = StandardScaler()\n",
    "scaled_data_X = scaler.fit_transform(X)\n",
    "scaled_data_Xtest= scaler.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA dimensionality reduction, v. 1\n",
    "\n",
    "Now that you have the data, use the `PCA` class to reduce the dimensionality of the dataset. \n",
    "\n",
    "First, reduce the training data to 2 dimensions, saving it in a variable named `X2D`. Then, print out the values of the `explained_variance_ratio_` variable, the sum of those variables, and plot the `X2D` data, with class zero (non-fraudulent) data with green dots and the class one (fraudulent) data with red dots. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(227845, 2)\n",
      "Explained variance ratio: [0.06503317 0.05603072]\n",
      "Variance captured: 0.12106388924976397\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# This usage of PCA allows us to specify the number of dimensions\n",
    "# for the projection -- in this case, 2. \n",
    "pca = PCA(n_components=2)\n",
    "X2D = pca.fit_transform(scaled_data_X)\n",
    "\n",
    "# verify the projection changed the shape\n",
    "print(X2D.shape)\n",
    "\n",
    "# explained_variance_ratio_ is an array, containing the amount of total \n",
    "# variance in the original data maintained in each dimension of the \n",
    "# projection\n",
    "print('Explained variance ratio:', pca.explained_variance_ratio_)\n",
    "\n",
    "# if we add up all the values, it shows us how much of the total variance\n",
    "# in the original data has been kept\n",
    "print('Variance captured:', np.sum(pca.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of the projection\n",
    "\n",
    "How much of the variance in the data did projecting into two dimensions maintain? Given this value, and the plot of the projected data, how suitable do you think the projected data is for this classification task? Compose your answer in this cell. \n",
    "\n",
    "There seems to be almost no amount of variance captured in the 2D data set. This is not very good, as variance assists us in creating a more accutate model, so long as we don't use too much variance, as that also can cause innacuracies\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA dimensionality reduction, v. 2\n",
    "\n",
    "Now, use the `PCA` class to project the data, keeping at least 95% of the variance in the original dataset. Save the projected data in a variable named `X95`. Then, print out the values in the `explained_variance_ratio_` variable, the number of values in the variable, and their sum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(227845, 27)\n",
      "Explained variance ratio: [0.06503696 0.05604525 0.03468536 0.03431399 0.03395314 0.03381786\n",
      " 0.03378723 0.0336654  0.03354192 0.03351819 0.03347976 0.03343261\n",
      " 0.03340262 0.03338657 0.03337408 0.03333257 0.03328805 0.0332695\n",
      " 0.03325058 0.03321687 0.03318254 0.03312888 0.03310357 0.03300907\n",
      " 0.03289064 0.03286505 0.03274079]\n",
      "Variance captured: 0.9567190240430905\n"
     ]
    }
   ],
   "source": [
    "# specifying a float for n_components tells PCA to do whatever projection\n",
    "# needed to maintain that much variance.\n",
    "pca = PCA(n_components=0.95)\n",
    "X95= pca.fit_transform(scaled_data_X)\n",
    "\n",
    "# let's see how many dimensions were required for maintain 97% of the\n",
    "# total variance\n",
    "print(X95.shape)\n",
    "\n",
    "print('Explained variance ratio:', pca.explained_variance_ratio_)\n",
    "print('Variance captured:', np.sum(pca.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of the projection\n",
    "\n",
    "How many dimensions were required to keep at least 95% of the variance in the original dataset? Based on your answers to this question and the previous one, how valuable do you think PCA will be in this situation? Compose your anwers in this cell.\n",
    "\n",
    "\n",
    "From what it looks like, the required amount of dimensions in order to keep at least 95% of the variance in the original dataset is 27 dimensions. \n",
    "\n",
    "In this situation, I believe that PCA will be pretty valuable, because it lets us access a large number of dimensions in our work, and it allows us to get the variance we need for a useful model when it comes to Machine Learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
