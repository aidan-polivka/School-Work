{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IST 371 Machine Learning\n",
    "\n",
    "## Chapter 14 PA\n",
    "\n",
    "### Spring 2019\n",
    "\n",
    "### Alex Lange, Aidan Polivka\n",
    "\n",
    "This assignment uses an RNN to classify SMS messages (texts) as spam or \"ham,\" i.e., non-spam. Use the provided `sms-spam.csv` file as your dataset, and fill in the following code cells, using the sarcasm detection example as a basis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is-spam</th>\n",
       "      <th>sms</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Go until jurong point\\r\\n0,Ok lar... Joking wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>FreeMsg Hey there darling it's been 3 week's n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>SIX chances to win CASH! From 100 to 20\\r\\n1,U...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>I've been searching for the right words to tha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>I HAVE A DATE ON SUNDAY WITH WILL!!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   is-spam                                                sms\n",
       "0        0  Go until jurong point\\r\\n0,Ok lar... Joking wi...\n",
       "1        1  FreeMsg Hey there darling it's been 3 week's n...\n",
       "2        1  SIX chances to win CASH! From 100 to 20\\r\\n1,U...\n",
       "3        0  I've been searching for the right words to tha...\n",
       "4        0                I HAVE A DATE ON SUNDAY WITH WILL!!"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in data and display the first few lines\n",
    "import pandas as pd\n",
    "\n",
    "spamhamdata = pd.read_csv('sms-spam.csv')\n",
    "\n",
    "spamhamdata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is-spam</th>\n",
       "      <th>sms</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>go until jurong point\\r\\n0,ok lar... joking wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>freemsg hey there darling it's been 3 week's n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>six chances to win cash! from 100 to 20\\r\\n1,u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>i've been searching for the right words to tha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>i have a date on sunday with will!!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   is-spam                                                sms\n",
       "0        0  go until jurong point\\r\\n0,ok lar... joking wi...\n",
       "1        1  freemsg hey there darling it's been 3 week's n...\n",
       "2        1  six chances to win cash! from 100 to 20\\r\\n1,u...\n",
       "3        0  i've been searching for the right words to tha...\n",
       "4        0                i have a date on sunday with will!!"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lowercase the sms column, and display the first few\n",
    "# rows of the dataframe\n",
    "spamhamdata['sms'] = spamhamdata['sms'].apply(lambda x: x.lower())\n",
    "spamhamdata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Alex\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Alex\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an', 'and', 'any', 'are', 'aren', \"aren't\", 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'can', 'couldn', \"couldn't\", 'd', 'did', 'didn', \"didn't\", 'do', 'does', 'doesn', \"doesn't\", 'doing', 'don', \"don't\", 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', 'hadn', \"hadn't\", 'has', 'hasn', \"hasn't\", 'have', 'haven', \"haven't\", 'having', 'he', 'her', 'here', 'hers', 'herself', 'him', 'himself', 'his', 'how', 'i', 'if', 'in', 'into', 'is', 'isn', \"isn't\", 'it', \"it's\", 'its', 'itself', 'just', 'll', 'm', 'ma', 'me', 'mightn', \"mightn't\", 'more', 'most', 'mustn', \"mustn't\", 'my', 'myself', 'needn', \"needn't\", 'no', 'nor', 'not', 'now', 'o', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 're', 's', 'same', 'shan', \"shan't\", 'she', \"she's\", 'should', \"should've\", 'shouldn', \"shouldn't\", 'so', 'some', 'such', 't', 'than', 'that', \"that'll\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', 'these', 'they', 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 've', 'very', 'was', 'wasn', \"wasn't\", 'we', 'were', 'weren', \"weren't\", 'what', 'when', 'where', 'which', 'while', 'who', 'whom', 'why', 'will', 'with', 'won', \"won't\", 'wouldn', \"wouldn't\", 'y', 'you', \"you'd\", \"you'll\", \"you're\", \"you've\", 'your', 'yours', 'yourself', 'yourselves']\n",
      "0 ['go', 'jurong', 'point\\r\\nok', 'lar', 'joking', 'wif', 'u', 'oni\\r\\nfree', 'entry', '', 'wkly', 'comp', 'win', 'fa', 'cup', 'final', 'tkts', 'st', 'may', '', 'text', 'fa', '', 'receive', 'entry', 'questionstd', 'txt', 'ratetcs', 'apply', 'overs\\r\\nu', 'dun', 'say', 'early', 'hor', 'u', 'c', 'already', 'say\\r\\nnah', 'dont', 'think', 'go', 'usf']\n",
      "1 ['freemsg', 'hey', 'darling', '', 'week', 'word', 'back', 'id', 'like', 'fun', 'still', 'tb', 'ok', 'xxx', 'std', 'chgs', 'send\\r\\neven', 'brother', 'like', 'speak', 'treat', 'like', 'aid', 'patent\\r\\nas', 'per', 'request', 'melle', 'melle', 'oru', 'minnaminunginte', 'nurungu', 'vettam', 'set', 'callertune', 'caller', 'press', '', 'copy', 'friend', 'callertune\\r\\nwinner', 'valued', 'network', 'customer', 'selected', 'receivea', '', 'prize', 'reward', 'claim', 'call', '', 'claim', 'code', 'kl', 'valid', '', 'hour', 'only\\r\\nhad', 'mobile', '', 'month', 'u', 'r', 'entitled', 'update', 'latest', 'colour', 'mobile', 'camera', 'free', 'call', 'mobile', 'update', 'co', 'free', '\\r\\nim', 'gonna', 'home', 'soon', 'dont', 'want', 'talk', 'stuff', 'anymore', 'tonight']\n",
      "2 ['six', 'chance', 'win', 'cash', '', '\\r\\nurgent', '', 'week', 'free', 'membership', '']\n",
      "3 ['ive', 'searching', 'right', 'word', 'thank', 'breather', 'promise', 'wont', 'take', 'help', 'granted', 'fulfil', 'promise', 'wonderful', 'blessing', 'time']\n",
      "4 ['date', 'sunday']\n",
      "5 ['xxxmobilemovieclub', 'use', 'credit\\r\\noh', 'kim', 'watching', 'here\\r\\neh', 'u', 'remember', '', 'spell', 'name', 'yes', 'v', 'naughty', 'make', 'v', 'wet\\r\\nfine', 'thats', 'way', 'u', 'feel', 'thats', 'way', 'gota', 'b\\r\\nengland', 'v', 'macedonia', '', 'dont', 'miss', 'goalsteam', 'news', 'txt', 'ur', 'national', 'team', '', 'eg', 'england', '', 'trywales']\n",
      "6 ['seriously', 'spell', 'name']\n",
      "7 ['im', 'going', 'try', '', 'month', 'ha', 'ha', 'joking']\n",
      "8 ['', 'pay', 'first', 'lar', 'da', 'stock', 'comin']\n",
      "9 ['aft', 'finish', 'lunch', 'go', 'str', 'lor', 'ard', '', 'smth', 'lor', 'u', 'finish', 'ur', 'lunch', 'already']\n",
      "10 ['ffffffffff', 'alright', 'way', 'meet', 'sooner']\n",
      "11 ['forced', 'eat', 'slice', 'im', 'really', 'hungry', 'tho', 'suck', 'mark', 'getting', 'worried', 'know', 'im', 'sick', 'turn', 'pizza', 'lol']\n",
      "12 ['lol', 'always', 'convincing']\n",
      "13 ['catch', 'bus', '', 'frying', 'egg', '', 'make', 'tea', 'eating', 'mom', 'left', 'dinner', '', 'feel', 'love', '']\n",
      "14 ['im', 'back', 'amp', 'packing', 'car', 'now\\r\\nahhh', 'work', 'vaguely', 'remember', 'feel', 'like', 'lol\\r\\nwait', 'thats', 'still', 'clear']\n",
      "15 ['yeah', 'got', '', 'v', 'apologetic', 'n', 'fallen', 'actin', 'like', 'spoilt', 'child', 'got', 'caught', 'till', '', 'wont', 'go', 'badly', 'cheer', '']\n",
      "16 ['k', 'tell', 'anything']\n",
      "17 ['fear', 'fainting', 'housework', 'quick', 'cuppa']\n",
      "18 ['thanks', 'subscription', 'ringtone', 'uk', 'mobile', 'charged', 'month', 'please', 'confirm', 'replying', 'yes', 'reply', 'charged']\n",
      "19 ['yup', 'ok', 'go', 'home', 'look', 'timing', 'msg', '', 'xuhui', 'going', 'learn', 'nd', 'may', 'lesson']\n"
     ]
    }
   ],
   "source": [
    "# perform other data clean up (removing non-alpha characters, \n",
    "# stopwords, and lemmatizing), then print out the first few\n",
    "# lines of the resulting array\n",
    "\n",
    "import nltk\n",
    "# preparation for step 3 -- download and load stopwords\n",
    "nltk.download('stopwords') # only need to run once\n",
    "# get a list of english stopwords\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "# display the stopwords \n",
    "print(sorted(stopwords))\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemm = WordNetLemmatizer()\n",
    "\n",
    "def headlineToList(headline):\n",
    "    # split into individual words\n",
    "    headline = headline.split(' ')\n",
    "    # remove non-alpha characters using a regular expression.\n",
    "    # ^a-zA-Z\\s matches any character that's not alphabetic or\n",
    "    # a space; matching characters are removed\n",
    "    headline = [re.sub('[^a-zA-Z\\s]', '', word) for word in headline]\n",
    "    # only keep words that are not stopwords\n",
    "    headline = [word for word in headline if word not in stopwords]\n",
    "    # lemmatize words\n",
    "    headline = [lemm.lemmatize(word) for word in headline]\n",
    "    return headline\n",
    "\n",
    "# 3. remove stopwords\n",
    "# 4. remove non-alphabetic characters\n",
    "# 5. Lemmatize\n",
    "nltk.download('wordnet') #This is also a one of download.\n",
    "# headlines will be a list of lists.\n",
    "# Each list in headlines contains the words\n",
    "# of each sms, minus stopwords and non-alpha\n",
    "# characters\n",
    "headlines = []\n",
    "\n",
    "# loop thru the sms column\n",
    "for row in range(0, spamhamdata.shape[0]):\n",
    "    # grab the current sms\n",
    "    headline = spamhamdata.sms[row]\n",
    "    \n",
    "    # append to the list\n",
    "    headlines.append(headlineToList(headline))\n",
    "\n",
    "    \n",
    "# headlines is a list, not a Pandas dataframe, so we don't have\n",
    "# a head() function. So, let's use a loop to display the first\n",
    "# 20 processed headlines\n",
    "for i in range(20):\n",
    "    print(i, headlines[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8859 distinct words in the dataset\n"
     ]
    }
   ],
   "source": [
    "# use a set to determine the number of distinct words in \n",
    "# the sms data\n",
    "wordSet = set()\n",
    "\n",
    "# populate the set\n",
    "for line in headlines:\n",
    "    for word in line:\n",
    "        wordSet.add(word)\n",
    "        \n",
    "# display number of distince words in the headlines data\n",
    "print(len(wordSet), 'distinct words in the dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# tokenize the data and pad it so each row is of\n",
    "# same length \n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# now we will convert the words in the headlines into \n",
    "# numeric values\n",
    "\n",
    "# maximum number of words to tokenize. We could make this\n",
    "# smaller (say, 1000 or 2000). Training would be faster, but\n",
    "# we might lose some important information (the rarer words \n",
    "# would be ignored). \n",
    "max_words = 8900   \n",
    "\n",
    "# create a tokenizer\n",
    "tokenizer = Tokenizer(num_words = max_words)\n",
    "\n",
    "# fit the tokenizer to our data\n",
    "tokenizer.fit_on_texts(headlines)\n",
    "\n",
    "# convert each headline to a sequence of words\n",
    "X = tokenizer.texts_to_sequences(headlines)\n",
    "\n",
    "# make each tokenized headline the same length\n",
    "# (shorter ones are left-filled with the value 0)\n",
    "X = pad_sequences(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       ...,\n",
       "       [0, 1],\n",
       "       [1, 0],\n",
       "       [1, 0]], dtype=uint8)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert the 'is-spam' labels to one-hot format using\n",
    "# the pd.get_dummies() function\n",
    "y = pd.get_dummies(spamhamdata['is-spam']).values\n",
    "\n",
    "# look at the new labels\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide into testing and training data\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "# divide into training and non-training data\n",
    "splitter = StratifiedShuffleSplit(n_splits = 1, test_size = 0.2, random_state = 68333)\n",
    "for train_index, test_index in splitter.split(X, y):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Alex\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\Alex\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "# define your RNN model, and compile it\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Embedding, GlobalMaxPool1D, GRU, LSTM\n",
    "from keras.backend import clear_session\n",
    "\n",
    "# using Keras vs. the TensorFlow Keras; same effect\n",
    "clear_session()\n",
    "\n",
    "# set up the model\n",
    "model = Sequential([\n",
    "    Embedding(input_dim = max_words, output_dim = 150, input_length = X.shape[1]),\n",
    "    LSTM(units = 256, dropout = 0.2, recurrent_dropout = 0.2),\n",
    "    Dense(256, activation = 'elu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(128, activation = 'elu'),\n",
    "    Dropout(0.5), \n",
    "    Dense(56, activation = 'elu'),\n",
    "    Dropout(0.5), \n",
    "    Dense(2, activation = 'softmax')\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    loss = 'categorical_crossentropy', \n",
    "    optimizer = 'adam', \n",
    "    metrics = ['accuracy']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# create a checkpointer that will save the best ever \n",
    "# epoch's weights; so, if the accuracy goes down \n",
    "# slightly as the training progresses, we will have the\n",
    "# best-ever model saved\n",
    "checkpointer = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath = 'best_weights.hdf5',\n",
    "    monitor = 'val_acc',\n",
    "    verbose = 1,\n",
    "    save_best_only = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Alex\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 1852 samples, validate on 464 samples\n",
      "Epoch 1/10\n",
      "1852/1852 [==============================] - 27s 15ms/step - loss: 0.3300 - acc: 0.8909 - val_loss: 0.2326 - val_acc: 0.9138\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.91379, saving model to best_weights.hdf5\n",
      "Epoch 2/10\n",
      "1852/1852 [==============================] - 32s 17ms/step - loss: 0.1414 - acc: 0.9487 - val_loss: 0.2637 - val_acc: 0.9138\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.91379\n",
      "Epoch 3/10\n",
      "1852/1852 [==============================] - 35s 19ms/step - loss: 0.0506 - acc: 0.9833 - val_loss: 0.3418 - val_acc: 0.9073\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.91379\n",
      "Epoch 4/10\n",
      "1852/1852 [==============================] - 31s 17ms/step - loss: 0.0223 - acc: 0.9919 - val_loss: 0.4409 - val_acc: 0.9009\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.91379\n",
      "Epoch 5/10\n",
      "1852/1852 [==============================] - 29s 15ms/step - loss: 0.0189 - acc: 0.9935 - val_loss: 0.5107 - val_acc: 0.9052\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.91379\n",
      "Epoch 6/10\n",
      "1852/1852 [==============================] - 29s 16ms/step - loss: 0.0148 - acc: 0.9973 - val_loss: 0.5623 - val_acc: 0.8944\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.91379\n",
      "Epoch 7/10\n",
      "1852/1852 [==============================] - 29s 16ms/step - loss: 0.0059 - acc: 0.9989 - val_loss: 0.6388 - val_acc: 0.9073\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.91379\n",
      "Epoch 8/10\n",
      "1852/1852 [==============================] - 29s 16ms/step - loss: 0.0013 - acc: 0.9995 - val_loss: 0.7401 - val_acc: 0.9116\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.91379\n",
      "Epoch 9/10\n",
      "1852/1852 [==============================] - 30s 16ms/step - loss: 0.0077 - acc: 0.9989 - val_loss: 0.6432 - val_acc: 0.9095\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.91379\n",
      "Epoch 10/10\n",
      "1852/1852 [==============================] - 29s 16ms/step - loss: 0.0077 - acc: 0.9995 - val_loss: 0.6298 - val_acc: 0.9073\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.91379\n"
     ]
    }
   ],
   "source": [
    "# train the model \n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs = 10,\n",
    "    batch_size = 32,\n",
    "    validation_split = 0.2, # use 20% of training data for validation\n",
    "    callbacks = [checkpointer],\n",
    "    verbose = 1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reinitialize the model with the best weights,\n",
    "# and save the model so we could use it later\n",
    "model.load_weights('best_weights.hdf5')\n",
    "model.save('shapes_cnn.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "580/580 [==============================] - 2s 3ms/step\n",
      "Test loss: 0.2223778432813184\n",
      "Test accuracy: 0.9206896551724137\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model's performance on the test data\n",
    "score = model.evaluate(X_test, y_test, verbose=1)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
